{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEPtxamkHxF9"
      },
      "source": [
        "# Tutorial 3: Geometric Deep Learning with PyTorch\n",
        "\n",
        "Contact: [William Cappelletti](mailto:william.cappelletti@epfl.ch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbEQWtmIM5VG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNfq0JnfDtzx"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
        "\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-od6ArQuM25-"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCapUQwvrr3t"
      },
      "source": [
        "To activate a GPU in colab , open the *Runtime* drop-down menu and click *Change runtime type*, then choose GPU as hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NsGBescrb9r"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5todSm-MAdS"
      },
      "source": [
        "## Part 1: PyTorch\n",
        "\n",
        "[PyTorch](https://pytorch.org/) is a open source machine learning framework.\n",
        "\n",
        "Is is based on tensor manipulation and automatic differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka5SoqIhOGfI"
      },
      "source": [
        "### Tensor manipulation\n",
        "\n",
        "Torch tensors can be manipulated just like numpy arrays, and they support all algebraic manipulations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fspQdxj7HpRS"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor(\n",
        "    [[1, 0],\n",
        "     [0, 2]]\n",
        ")\n",
        "\n",
        "b = torch.tensor(\n",
        "    [[0, 3],\n",
        "     [4, 0]]\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "\n",
        "print(\"a + b:\")\n",
        "print(a + b)\n",
        "\n",
        "print(\"a * b\")\n",
        "print(a * b)\n",
        "\n",
        "print(\"a @ b\")\n",
        "print(a @ b)\n",
        "\n",
        "print(\"2.5 * a\")\n",
        "print(2.5 * a)\n",
        "\n",
        "print(\"1 + a\")\n",
        "print(1 + a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iuXuTDMOlY3"
      },
      "source": [
        "### PyTorch modules\n",
        "\n",
        "Neural networks are defined in PyTorch through [`Modules`](https://pytorch.org/docs/stable/notes/modules.html), which implement `forward` and `backward` methods.\n",
        "The `forward` methods takes batches of inputs and performs some differentiable manipulation.\n",
        "The backward pass compute the gradient of `forward` by automatic differentiation.\n",
        "\n",
        "The `torch.nn` module provides many popular neural network layers, which can be composed out of the box. You can have a look at the [documentation](https://pytorch.org/docs/stable/nn.html) for a complete list of available layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDJXD_KHPb2J"
      },
      "outputs": [],
      "source": [
        "# Let's generate a random design matrix\n",
        "X = torch.empty(5, 10).normal_()\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXrgN3MwNc4P"
      },
      "outputs": [],
      "source": [
        "# Let's apply a Linear (fully connected) layer\n",
        "linear = nn.Linear(\n",
        "    in_features=10,\n",
        "    out_features=2,\n",
        ")\n",
        "\n",
        "out = linear(X)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtlFwMgNRFi2"
      },
      "source": [
        "We can define custom modules by inheriting from the `toch.nn.Module` base class and defining the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMIyf3v8QHsm"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return (input @ self.weight) + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4Kz7HWwSH-e"
      },
      "outputs": [],
      "source": [
        "my_linear = MyLinear(10, 2)\n",
        "my_linear(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HbXi6P6TDkF"
      },
      "source": [
        "### Neural networks\n",
        "\n",
        "Feedforward neural networks are, to simplify, a stack of layers which maps from input to output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YZTPMVHfmiQ"
      },
      "outputs": [],
      "source": [
        "mlp = nn.Sequential(\n",
        "    nn.Linear(10,20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 2),\n",
        "    nn.Softmax(dim=-1),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtRixhw6hJpv"
      },
      "outputs": [],
      "source": [
        "mlp(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP7AbufAhnzk"
      },
      "source": [
        "For more flexibility, we can define NNs as Modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT_0rmLqhLIo"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, nb_in: int, nb_hidden: int, nb_out: int) -> None:\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(nb_in, nb_hidden)\n",
        "        self.l2 = nn.Linear(nb_hidden, nb_hidden)\n",
        "        self.l3 = nn.Linear(nb_hidden, nb_out)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        X = torch.relu(self.l1(X))\n",
        "        X = torch.relu(self.l2(X))\n",
        "        return torch.softmax(self.l3(X), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJvB6qc5i8sh"
      },
      "outputs": [],
      "source": [
        "mlp2 = MLP(10, 20, 2)\n",
        "mlp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_y5ldY3jEDG"
      },
      "outputs": [],
      "source": [
        "mlp2(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXTftHh2rqUa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvL3-WxVjtFC"
      },
      "source": [
        "### Using GPUs\n",
        "\n",
        "All tensors and modules can be moved to GPU using the `.to(...)` method. Pay attention, everything shall be moved to the same \"device\"!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIEBxKQCjQuu"
      },
      "outputs": [],
      "source": [
        "X = X.to(device)\n",
        "mlp.to(device)\n",
        "\n",
        "\n",
        "# Verify the device of your objects\n",
        "print(f\"X: {X.device}, output: {mlp(X).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Ru3-sGmG7W"
      },
      "source": [
        "## Part 2: Training neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLm_qZ7WmRUi"
      },
      "source": [
        "### Datasets and dataloaders\n",
        "\n",
        "PyTorch provides utilities to load and batch data. They also have many datasets ready to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXoLhub5l9g9"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjWX6LZAFeif"
      },
      "source": [
        "We use the MNIST dataset for this part of the tutorial. It is readily available in PyTorch, and quickly downloaded on Google's servers ðŸ˜‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2iCXEBdrIC_"
      },
      "outputs": [],
      "source": [
        "# We convert images to tensors and normalize them\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.1307,), (0.3081,))] # Mean and std are well known\n",
        ")\n",
        "dataset_tr = MNIST(\".\", transform=transform, train=True, download=True)        \n",
        "dataset_te = MNIST(\".\", transform=transform, train=False, download=True)        \n",
        "\n",
        "# Lets wrap the dataset in a dataloader, to get batched samples\n",
        "BATCH_SIZE = 64\n",
        "loader_tr = DataLoader(dataset_tr, batch_size=BATCH_SIZE)\n",
        "loader_te = DataLoader(dataset_te, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swYnkxWyGJik"
      },
      "source": [
        "MNIST dataset contains 28x28 grayscale images of handwritten digits, which we would like to identify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGGhnPbzrB4W"
      },
      "outputs": [],
      "source": [
        "def plot_random_sample(ax):\n",
        "    X, y = dataset_tr[torch.randint(len(dataset_tr), (1,)).item()]\n",
        "    \n",
        "    ax.imshow(X[0].numpy()*0.3081+0.1307)\n",
        "    ax.set(title=y)\n",
        "        \n",
        "fig, axes = plt.subplots(1,5, figsize=(15,3))\n",
        "for ax in axes:\n",
        "    plot_random_sample(ax)\n",
        "    ax.set(xticks=[], yticks=[])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZDL5EYTNhDn"
      },
      "source": [
        "### Loss functions\n",
        "\n",
        "PyTorch comes with many loss functions in the `torch.nn` module. For a complete list you can refer to the [doc](https://pytorch.org/docs/stable/nn.html#loss-functions).\n",
        "\n",
        "For our handwritten-digit classification task a good choice is the [cross entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss), which takes raw class scores and  compares them to the desired class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de7omhTTNgk3"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_fn(\n",
        "    torch.tensor(\n",
        "        [[ 0, 10,  2],\n",
        "         [11,  6,  5],\n",
        "         [13, 20,  1],\n",
        "         [ 2,  7, 15]],\n",
        "        dtype=torch.float\n",
        "    ),\n",
        "     torch.tensor([1, 0, 0, 2])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jklIgRdH6zK"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Neural Networks are trained with Stochastic Gradient Descent (SGD), or variants thereof. One \"epoch\" of SGD consist in performing sequentially one descent step for each batch in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Ca6YIZuJVe"
      },
      "outputs": [],
      "source": [
        "# Lets define a feedforward CNN\n",
        "\n",
        "cnn = nn.Sequential(\n",
        "    nn.Conv2d(1, 4, 5), # out_size: (4, 24, 24)\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(4, 16, 5), # out_size: (16, 20, 20)\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(16, 16, 5), # out_size: (16, 16, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten(), # out_size: (4096),\n",
        "    nn.Linear(4096, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 10),\n",
        ").to(device)\n",
        "\n",
        "# Note that we moved it to GPU!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-YRduDyQvxZ"
      },
      "source": [
        "SGD is an optimization algorithm and PyTorch implements it, along with many others, in the `torch.optim` module (cfr [doc](https://pytorch.org/docs/stable/optim.html)).\n",
        "Those \"optimizers\" handle the update of the desired parameters through a `step()` method which they all implement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inmjS7lVVE4e"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(\n",
        "    params=cnn.parameters(),\n",
        "    lr=1e-3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SgZooteMEEC"
      },
      "outputs": [],
      "source": [
        "def train_step(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: nn.Module,\n",
        "    X: torch.Tensor,\n",
        "    y: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    # Start by resetting the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Compute the model predictions and their loss\n",
        "    out = model(X)\n",
        "    loss = loss_fn(out, y)\n",
        "\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Perform the optimization step\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpyv1WtkUNiQ"
      },
      "source": [
        "Now let's optimize our CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5TlTIRdUJhx"
      },
      "outputs": [],
      "source": [
        "NB_EPOCHS = 5\n",
        "nb_batches = round(len(dataset_tr) / BATCH_SIZE)\n",
        "    \n",
        "for epoch in range(NB_EPOCHS):\n",
        "    for X, y in tqdm(loader_tr, total=nb_batches, desc=f\"Epoch {epoch+1}\"):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "    \n",
        "        loss = train_step(cnn, optimizer, loss_fn, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC2XZqKVYGll"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now that the model is trained, we shall check its performances. To compute metrics, we use the [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/) library, which we already installed in the setup.\n",
        "\n",
        "We are mainly interested in the accuracy of our model, so lets compute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXEhGvT9W9Ue"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, metric: torchmetrics.Metric, loader: DataLoader):\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(loader, total=len(loader)):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            # Compute probabilities\n",
        "            preds = model(X).softmax(dim=-1)\n",
        "    \n",
        "            # metric on current batch\n",
        "            _ = metric(preds, y)\n",
        "        \n",
        "        # Accumated metric (on all batches)\n",
        "        return metric.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkEMJZ7ca1s4"
      },
      "outputs": [],
      "source": [
        "accuracy_fn = torchmetrics.Accuracy().to(device)\n",
        "acc_tr = evaluate(cnn, accuracy_fn, loader_tr)\n",
        "\n",
        "accuracy_fn.reset() # Reset the accuracy!\n",
        "acc_te = evaluate(cnn, accuracy_fn, loader_te)\n",
        "\n",
        "print(f\"Train accuracy: {acc_tr:.2%}\")\n",
        "print(f\"Test accuracy:  {acc_te:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnoF1eaIebzT"
      },
      "source": [
        "Not bad for a first try! ðŸŽ‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWV_tPl2eBv2"
      },
      "source": [
        "## Part 3: PyTorch Geometric\n",
        "\n",
        "Now that we are acquainted with PyTorch, let's plug in Graph data! ðŸ˜\n",
        "\n",
        "Our framework of choice is [PyTorch Geometric][1], a useful extension of PyTorch designed to work with graphs. This tutorial will be a quick overview of a few main features, and you can complete it by checking out the [documentation][1] and the [official tutorials][2].\n",
        "\n",
        "\n",
        "[1]: https://pytorch-geometric.readthedocs.io/en/latest/\n",
        "[2]: https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn2vPXp5fgS6"
      },
      "source": [
        "### PyG data\n",
        "\n",
        "A single graph in PyTorch Geometric is described by an instance of `torch_geometric.data.Data`, which can hold several attributes, among which:\n",
        "\n",
        "* `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`\n",
        "\n",
        "* `data.edge_index`: Graph connectivity tensor with shape `[2, num_edges]` and type `torch.long`. Note that one would expect the **node pairs** to be saved as rows, but in this format they are **saved as columns** of the `edge_index` matrix.\n",
        "\n",
        "* `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`\n",
        "\n",
        "* `data.y`: Target to train against, e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]`\n",
        "\n",
        "Not all of these fields must be present and the user can define his own attributes. The following cell shows an example for the creation of a simple graph in PyTorch Geometric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oMGMK9sf0KM"
      },
      "outputs": [],
      "source": [
        "import torch_geometric as pyg\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emi5a2NgbsK8"
      },
      "outputs": [],
      "source": [
        "## Create an undirected graph having three nodes (0, 1, 2).\n",
        "## Nodes 0 and 1 are connected; nodes 1 and 2 are connected (in both directions).\n",
        "## Each node has a single feature. These features are equal to\n",
        "## -1, 0, 1 for the three nodes respectively, in this order.\n",
        "edge_index = torch.tensor([[0, 1, 1, 2],\n",
        "                           [1, 0, 2, 1]], dtype=torch.long)\n",
        "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "\n",
        "G1 = Data(x=x, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGxW8e92naoo"
      },
      "source": [
        "As you can see, it is very close to Networkx, and utility functions can map back and forth from one to the other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djr7Hutdgspd"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_networkx, from_networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkknRZYxoQGc"
      },
      "outputs": [],
      "source": [
        "Gnx = to_networkx(G1, to_undirected=True)\n",
        "\n",
        "fig, axes = plt.subplots(1,2, figsize=(6,3))\n",
        "nx.draw(Gnx, ax=axes[0])\n",
        "\n",
        "Gnx.add_node(3)\n",
        "Gnx.add_edges_from([(1,3), (2,3)])\n",
        "nx.draw(Gnx, ax=axes[1])\n",
        "plt.show()\n",
        "\n",
        "G2 = from_networkx(Gnx)\n",
        "print(G2.edge_index)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1sqCo3NqBPQ"
      },
      "source": [
        "### PyG Datasets and Dataloaders\n",
        "\n",
        "PyG implements its own wrappers of `Dataset` and `DataLoader`.\n",
        "They provide the same interface as regular PyTorch, but instead of Tensors they return instances of `Data`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01m9Ih0vXywV"
      },
      "source": [
        "PyG comes with some ready-to-use datasets, such as the Karate Club."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-y9ntzpU8gl"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = TUDataset(\n",
        "    root=\".\", name=\"Mutagenicity\",\n",
        ")\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "# Let's draw one sample\n",
        "nx.draw_kamada_kawai(to_networkx(dataset[0], to_undirected=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXvVWMN3Y44w"
      },
      "source": [
        "Custom datasets can be defined as well, by inheriting from the `torch_geometric.data.Dataset` class and defining the following methods:\n",
        "\n",
        "- `torch_geometric.data.Dataset.len()`: Returns the number of examples in your dataset.\n",
        "- `torch_geometric.data.Dataset.get()`: Implements the logic to load a single graph.\n",
        "\n",
        "\n",
        "\n",
        "See [\"Creating Your Own Datasets\"][^1] for more info.\n",
        "\n",
        "[^1]: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rxKZvBMZFLl"
      },
      "outputs": [],
      "source": [
        "class RandomGraphs(pyg.data.Dataset):\n",
        "    def __init__(self, nb_graphs: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.graph_list = []\n",
        "        for _ in range(nb_graphs):\n",
        "            G = nx.erdos_renyi_graph(10, 0.4)\n",
        "            self.graph_list.append(from_networkx(G))\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.graph_list)\n",
        "    \n",
        "    def get(self, index: int) -> Data:\n",
        "        return self.graph_list[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-ro8Rhqc6Hh"
      },
      "outputs": [],
      "source": [
        "rand_graphs = RandomGraphs(5)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
        "nx.draw_circular(to_networkx(rand_graphs[0], to_undirected=True), ax=axes[0])\n",
        "nx.draw_circular(to_networkx(rand_graphs[3], to_undirected=True), ax=axes[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YBXL9QVdVaj"
      },
      "source": [
        "For efficiency reasons, a Pytorch Geometric Dataloader builds a batch of graphs by considering it as a unique big graph: the big graph adjacency matrix is built by stacking diagonally the adjacency matrices of the graphs in the batch (which are defined by `edge_index`); features and target matrices are concatenated in the node dimension. This composition allows differing number of nodes and edges over examples in one batch.\n",
        "\n",
        "$$  A =\n",
        "  \\begin{bmatrix}\n",
        "    A_{1} & & \\\\\n",
        "    & \\ddots & \\\\\n",
        "    & & A_{N}\n",
        "  \\end{bmatrix}, \\quad X = \\begin{bmatrix}\n",
        "X_1\\\\ \n",
        "\\vdots \\\\ \n",
        "X_N\n",
        "\\end{bmatrix}, \\quad Y = \\begin{bmatrix}\n",
        "Y_1\\\\ \n",
        "\\vdots\\\\\n",
        "Y_N\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "A batch of graphs will contain the `batch` attribute, which is a tensor whose length is equal to the number of nodes in the big graph obtained by the union of all the graphs in the batch. Such tensor maps each node to the index of the graph that nodes originally came from. Thus, this allows for recovering the original graphs from a batch coming form the dataloader.\n",
        "\n",
        "Luckily, you will not have to handle this, but it is good to understand it nonetheless.\n",
        "This batching strategy allows us to learn local models on huge graphs just by looking at $n$-hop neighborhoods, instead of loading all data, for instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkg-3uxcoh5l"
      },
      "outputs": [],
      "source": [
        "# Split datsset in train and test\n",
        "split_idx = round(len(dataset) * 0.7)\n",
        "dataset_tr = dataset[:split_idx]\n",
        "dataset_te = dataset[split_idx:]\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "loader_tr = pyg.loader.DataLoader(dataset_tr, batch_size=BATCH_SIZE, shuffle=True)\n",
        "loader_te = pyg.loader.DataLoader(dataset_te, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5i-L5WZpvnb"
      },
      "outputs": [],
      "source": [
        "data_batch = next(iter(loader_tr))\n",
        "print(data_batch)\n",
        "print(\"Number of graphs in the batch:\",data_batch.num_graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZJz-x2lC0O"
      },
      "source": [
        "### Graph Neural Networks\n",
        "\n",
        "Graph Neural Networks can be implemented just like regular Neural Network in PyTorch, i.e. as `nn.Module`s. One should be careful to include, in the forward pass, all arguments required by the Graph layers. Generally those are the `edge_index` and the `batch` attributes of data, on top of the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Baax8PssUYB"
      },
      "outputs": [],
      "source": [
        "def gnn_evaluate(gnn: nn.Module, metrics: List[torchmetrics.Metric], loader: DataLoader):\n",
        "    with torch.no_grad():\n",
        "        gnn.eval()\n",
        "        for batch in tqdm(loader, total=len(loader)):\n",
        "            batch.to(device)\n",
        "            # Compute probabilities\n",
        "            preds = gnn(batch.x, batch.edge_index, batch.batch).softmax(dim=-1)\n",
        "    \n",
        "            # metric on current batch\n",
        "            for metric in metrics:\n",
        "                _ = metric(preds, batch.y)\n",
        "        \n",
        "        # Accumated metric (on all batches)\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtZBlazudajN"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, num_node_features: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = pyg.nn.GraphConv(num_node_features, 64)\n",
        "        self.conv2 = pyg.nn.GraphConv(64, 64)\n",
        "        self.conv3 = pyg.nn.GraphConv(64, 128)\n",
        "        self.conv4 = pyg.nn.GraphConv(128, 128)\n",
        "        self.conv5 = pyg.nn.GraphConv(128, 256)\n",
        "\n",
        "        self.linear1 = nn.Linear(256, num_classes)\n",
        "    \n",
        "    def forward(self, x, edge_index, batch= None):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index).relu()\n",
        "        x = self.conv4(x, edge_index).relu()\n",
        "        x = self.conv5(x, edge_index).relu()\n",
        "\n",
        "        x = pyg.nn.global_add_pool(x, batch)\n",
        "        return self.linear1(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxgHlBosl7dF"
      },
      "outputs": [],
      "source": [
        "gnn = GNN(\n",
        "    num_node_features=dataset.num_node_features,\n",
        "    num_classes = 2,\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(gnn.parameters(), lr=1e-3)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_9vXHcxowFL"
      },
      "outputs": [],
      "source": [
        "NB_EPOCHS = 15\n",
        "\n",
        "for epoch in range(NB_EPOCHS):\n",
        "    for batch in tqdm(loader_tr, desc=f\"Epoch {epoch+1}\", leave=False):\n",
        "        batch.to(device)\n",
        "    \n",
        "        out = gnn(batch.x, batch.edge_index, batch.batch)\n",
        "        loss = loss_fn(out, batch.y.flatten())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlBKbYy0swFv"
      },
      "outputs": [],
      "source": [
        "accuracy_fn = torchmetrics.Accuracy().to(device)\n",
        "confusion_fn = torchmetrics.ConfusionMatrix(num_classes=2).to(device)\n",
        "\n",
        "metrics = [accuracy_fn, confusion_fn]\n",
        "\n",
        "gnn_evaluate(gnn, metrics, loader_tr)\n",
        "acc_tr = accuracy_fn.compute()\n",
        "conf_tr = confusion_fn.compute().cpu().numpy()\n",
        "\n",
        "# Reset the metrics!\n",
        "for metric in metrics:\n",
        "    metric.reset()\n",
        "\n",
        "gnn_evaluate(gnn, metrics, loader_te)\n",
        "acc_te = accuracy_fn.compute()\n",
        "conf_te = confusion_fn.compute().cpu().numpy()\n",
        "\n",
        "print(f\"Train accuracy: {acc_tr:.2%}\")\n",
        "print(conf_tr)\n",
        "print(f\"Test accuracy:  {acc_te:.2%}\")\n",
        "print(conf_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN6GbjuOspSo"
      },
      "source": [
        "Again, pretty cool results! ðŸ¥³"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIMNbWs15ZfM"
      },
      "source": [
        "## Part 4: Splitting graph data\n",
        "\n",
        "In the previous sections, we encontered training and test data, without explaining what they are, or how we obtain them.\n",
        "\n",
        "### Definitions\n",
        "\n",
        "- **Train data**: is the set of samples that we use during training. The model tries to learn the objective function by observing them and thus learns parameters $\\mathbf{\\Theta^*}$ that mininimize the loss function $f(\\mathcal{D}_{tr}, \\mathbf{\\Theta^*})$.\n",
        "- **Test data**: is an independent dataset $\\mathcal{D}_{te}$ which is never seen during training. We use it to estimate the *expected performance* of the trained model (i.e. the average performance on real world data).\n",
        "- **Validation data**: it is again an independent set of samples. It is used to select *hyperparameters*, i.e. parts of the models that are not optimized with gradient descent. Sometimes it is replaced by *cross-validation*.\n",
        "\n",
        "It is extremely important to always use a *virgin test dataset* to report performances of your models, which is why an independent validation dataset shall be used for model selection.\n",
        "\n",
        "Generally, when we are provided a dataset, we obtain these splits with a random partition. Based on the amount of data, we choose how much to use for each split. A common option is to use 30% for test, 20% for validation, and 50% for train.\n",
        "\n",
        "### Network data\n",
        "\n",
        "When working with network data, the extraction of the three splits depends on the kind of task that we want to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTk8IGZoBLU0"
      },
      "source": [
        "#### Graph-level tasks\n",
        "\n",
        "In this case samples are single graphs and thus we can easily partiotion the dataset by randomly assigning graphs to each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W500Dny_BjDh"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = TUDataset(\n",
        "    root=\".\", name=\"Mutagenicity\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMgl3n_WqOXI"
      },
      "outputs": [],
      "source": [
        "# Create a random number generator\n",
        "from numpy.random import default_rng\n",
        "\n",
        "rng = default_rng(seed=2106)\n",
        "\n",
        "# shuffle \n",
        "indices = np.arange(len(dataset))\n",
        "rng.shuffle(indices)\n",
        "dataset = dataset[indices]\n",
        "\n",
        "# Split\n",
        "train_end_idx = int(0.5 * len(dataset))\n",
        "val_end_idx = int(0.7 * len(dataset))\n",
        "\n",
        "dataset_tr = dataset[:train_end_idx]\n",
        "dataset_val = dataset[train_end_idx:val_end_idx]\n",
        "dataset_te = dataset[val_end_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbBr8ph-EF15"
      },
      "source": [
        "#### Node-level tasks\n",
        "\n",
        "For node-level tasks, each node is a sample, and often they come from a single graph.\n",
        "In this case, we obtain the splits using non-overlapping **masks**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM1G7A2THeko"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import KarateClub\n",
        "\n",
        "dataset = KarateClub()\n",
        "\n",
        "print(\"Nb samples:\", len(dataset))\n",
        "\n",
        "data = dataset[0]\n",
        "print(\"Nb nodes:\", data.num_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "has_syyZHrCa"
      },
      "outputs": [],
      "source": [
        "train_mask = np.zeros(data.num_nodes, dtype=bool)\n",
        "train_mask[:10] = True\n",
        "rng.shuffle(train_mask)\n",
        "\n",
        "# The test mask should be the negation of the train mask\n",
        "test_mask = ~train_mask\n",
        "\n",
        "# Verify that train and test are disjoint\n",
        "np.all(test_mask != train_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh2Yi_85H1bF"
      },
      "outputs": [],
      "source": [
        "# In training we compute the loss only only on the training data\n",
        "try:\n",
        "    model = ...\n",
        "    loss_fn = ...\n",
        "\n",
        "    out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
        "    loss = loss_fn(out[:, train_mask], data.y[train_mask])  # Compute the loss solely based on the training nodes.\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsFBgYwDMabo"
      },
      "source": [
        "> **Attention!** Shuffling masks can make them overlap. Always define the test mask as the opposite of the train mask. The validation mask should be extracted from the train.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u21rPu98H2RA"
      },
      "source": [
        "#### Edge-level tasks\n",
        "\n",
        "Similarly to the node-level setting, the samples are single edges, which generally live on the same graph.\n",
        "Again, we proceed by masking, but we shall make sure to remove edges in *both directions*! (If the graph is undirected.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYEa5HvOLJNO"
      },
      "outputs": [],
      "source": [
        "print(\"Nb edges:\", data.num_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnEuWH_VORxo"
      },
      "outputs": [],
      "source": [
        "train_mask = np.zeros(data.num_edges)\n",
        "train_mask[:int(.5 * data.num_edges)] = True\n",
        "rng.shuffle(train_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k32n3rfQQWBA"
      },
      "outputs": [],
      "source": [
        "# In training we only use the training edges\n",
        "try:\n",
        "    model = ...\n",
        "    loss_fn = ...\n",
        "\n",
        "    out = model(data.x, data.edge_index[:, train_mask])  # Perform a single forward pass.\n",
        "    loss = loss_fn(out[:, train_mask], data.y[train_mask])  # Compute the loss solely based on the training nodes.\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCVGG89bQshf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
